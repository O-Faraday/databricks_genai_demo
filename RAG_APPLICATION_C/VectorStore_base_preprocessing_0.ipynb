{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9c4bc5-7b29-4a77-a478-97b321aea7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deploy a RAG system with Mosaic AI Agent Evaluation and Lakehouse Applications\n",
    "\n",
    "In this chapter, you will build a **Databricks Docs Assistant** to help users answer questions about Databricks, using :  \n",
    "- Vector database / index, \n",
    "- LLM endpoint\n",
    "- MLFlow for tracking, deployment\n",
    "- Lakehouse for data housing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f3239b-2e3d-46f4-80c5-756d3c908dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## In this notebook, we will build the pipeline to feed the vectorstore database.\n",
    "\n",
    "Our goal is to develop an assistant able to read the docs of databricks to answer dev questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20444d3-ca22-4083-aa01-96334bddaa08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract and preprocess of the contextual documents. \n",
    "\n",
    "This notebook will be used to create a delta table in the unity Cat&alog that will be next used as vectorstore database.\n",
    "\n",
    "For this we will use two types of chunking method, \n",
    "- the first one with classic recursive chunking on a html source\n",
    "- the second with a docling extract + chunk.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7006e012-9f57-412b-add9-6948442ded11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Docling :\n",
    "Docling simplifies document processing, parsing diverse formats — including advanced PDF understanding — and providing seamless integrations with the gen AI ecosystem.\n",
    "\n",
    "It's an open-source project from IBM.\n",
    "\n",
    "This tool benefits from advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more.\n",
    "\n",
    "Ready for image integration with GraniteDocling that Supports several Visual Language Models.\n",
    "\n",
    "This at the time one of the most deployed repository from Github platform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e276c356-6066-4bb2-ac45-3963f9703075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet databricks-langchain==0.6.0 mlflow[databricks]==3.4.0  langchain==0.3.27 langchain_core==0.3.74 bs4 langchain_community markdownify docling\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028e8b4b-7820-49a7-97c9-bbc2c864f049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalog et schema \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f9f647-95ea-4c00-ba9c-6bd4f5cd2664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"demo\"\n",
    "schema = \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c59b5f-2d2f-4a30-abc6-9920d4ad490e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Source extract\n",
    "The source is a part of the doc QA sources of databricks docs.\n",
    "\n",
    "As we are limited in use volume, I will only keep the part of databricks docs about MLFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ef5e8e-086a-4b1f-8d38-579312a16bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1- Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81ad4f7-752d-4aee-94a4-b1bdf5a63ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABRICKS_SITEMAP_URL = \"https://docs.databricks.com/aws/en/sitemap.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c99d07-1a4d-4b7f-b6ff-9b4937dc54ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1-1 Recup of the pertinent URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba457bdf-e729-4607-bad8-ed6d1f3bbfe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# Constants\n",
    "URL_PARALLELISM = 50\n",
    "\n",
    "# Setup shared retry-enabled HTTP session\n",
    "shared_session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=3, status_forcelist=[429])\n",
    "adapter = HTTPAdapter(max_retries=retries, pool_maxsize=URL_PARALLELISM)\n",
    "shared_session.mount(\"http://\", adapter)\n",
    "shared_session.mount(\"https://\", adapter)\n",
    "\n",
    "# Fetch sitemap URLs\n",
    "def fetch_urls(filter_documents=None):\n",
    "    response = shared_session.get(DATABRICKS_SITEMAP_URL)\n",
    "    root = ET.fromstring(response.content)\n",
    "    urls = [loc.text for loc in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\") if filter_documents is None or filter_documents in loc.text]\n",
    "    return urls\n",
    "\n",
    "url_docs_mlflow = fetch_urls(filter_documents=\"/mlflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0f3549-2625-4788-9efc-b2e79c44f5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2- Create a table with classic chunks.  \n",
    "\n",
    "#### Extracting Databricks documentation sitemap and pages\n",
    "\n",
    "For this demo, we will directly download a few documentation pages from docs.databricks.com and extract the HTML content.\n",
    "\n",
    "Here are the main steps:\n",
    "\n",
    "    Run a quick script to extract the page URLs from the databricks_urls.csv file\n",
    "    Download the web pages\n",
    "    Use BeautifulSoup to extract the ArticleBody\n",
    "    \n",
    "\n",
    "#### Splitting documentation pages into small chunks\n",
    "\n",
    "LLM models typically have a maximum input context length, and you won't be able to compute embeddings for very long texts. In addition, the longer your context length is, the longer it will take for the model to provide a response.\n",
    "\n",
    "Document preparation is key for your model to perform well, and multiple strategies exist depending on your dataset:\n",
    "\n",
    "    Split document into small chunks (paragraph, h2...)\n",
    "    Truncate documents to a fixed length\n",
    "    The chunk size depends on your content and how you'll be using it to craft your prompt. Adding multiple small doc chunks in your prompt might give different results than sending only a big one\n",
    "    Split into big chunks and ask a model to summarize each chunk as a one-off job, for faster live inference\n",
    "    Create multiple agents to evaluate each bigger document in parallel, and ask a final agent to craft your answer...\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8509b6-a2f9-42b2-8cea-790601554f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_transformers import MarkdownifyTransformer\n",
    "\n",
    "\n",
    "md = MarkdownifyTransformer([\"script\", \"style\", \"nav\", \"footer\"])\n",
    "\n",
    "def custom_html_splitter(url_doc, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "        for the url_doc, fetch the HTML content, parse it with BeautifulSoup, and split it into smaller chunks.\n",
    "        chunk_size and chunk_overlap are parameters of RecursiveCharacterTextSplitter\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "        \n",
    "    response = requests.get(url_doc, headers=headers, timeout=30)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    doc = {\n",
    "        'content': soup.get_text(),\n",
    "        'metadata': {\"url\":url_doc}\n",
    "    }\n",
    "    \n",
    "  \n",
    "    # Create a LangChain document\n",
    "    documents = [Document(page_content=soup.get_text(), \n",
    "                         metadata={\"url\":url_doc}) \n",
    "                ]\n",
    "    \n",
    "    converted_docs = md.transform_documents(documents)\n",
    "    # Apply text splitter, you can add a separator if needed\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    return text_splitter.split_documents(converted_docs)\n",
    "\n",
    "\n",
    "\n",
    "def create_documents(databricks_urls) : \n",
    "    \"\"\"\n",
    "        for each url_doc, add the Documents results from custom_html_splitter\n",
    "        to the list of documents extracted form the html docs. \n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for databricks_url in databricks_urls :\n",
    "        documents += custom_html_splitter(databricks_url, chunk_size=1000)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = create_documents(databricks_urls=url_docs_mlflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28aadfdd-7529-4b92-8216-9f20b3b4e3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(documents[0].metadata)\n",
    "print(documents[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f6c97e-3fd7-4c28-b4c3-ba7b05599f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Convert the list of Chunks split with lanchain features to columns of text, doc.content, and url , doc.metadata.url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2aa5526-1f57-4075-99f9-023bf1e01ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# From the list of langchain chunks create data for the vectorstore table. \n",
    "data = [{\n",
    "            'content': doc.page_content,\n",
    "            'url': str(doc.metadata['url']),  \n",
    "            # Add as many information as you need\n",
    "        }\n",
    "        for doc in documents ]\n",
    "\n",
    "# Create a table in UC that can be used a vector store\n",
    "df_pandas = pd.DataFrame(data)\n",
    "df_spark = spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96cee1e-df9c-4b84-9f02-1ef92b1545a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1755869736668}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9932b6-88d3-4a2b-b5f8-d7cecc81b21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3- Launch the docling preparation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e2d05f-5cc1-406e-afe2-24703c66503c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from io import BytesIO\n",
    "from docling.backend.html_backend import HTMLDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.document import InputDocument\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "doc_convert = DocumentConverter()\n",
    "chunker = HybridChunker()\n",
    "data_docling = []\n",
    "for reference_url in url_docs_mlflow:\n",
    "\n",
    "    # example github : url = \"https://en.wikipedia.org/wiki/Duck\"\n",
    "    text = urllib.request.urlopen(reference_url).read()\n",
    "    in_doc = InputDocument(\n",
    "        path_or_stream=BytesIO(text),\n",
    "        format=InputFormat.HTML,\n",
    "        backend=HTMLDocumentBackend,\n",
    "        filename=reference_url,\n",
    "    )\n",
    "    backend = HTMLDocumentBackend(in_doc=in_doc, path_or_stream=BytesIO(text))\n",
    "    dl_doc = backend.convert().export_to_markdown()\n",
    "    #print(len(dl_doc))\n",
    "\n",
    "    doc = doc_convert.convert(source=reference_url).document\n",
    "\n",
    "\n",
    "    chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        #print(f\"=== {i} ===\")\n",
    "        #print(f\"chunk.text:\\n{f'{chunk.text[:300]}…'!r}\")\n",
    "\n",
    "        enriched_text = chunker.contextualize(chunk=chunk)\n",
    "        #print(f\"chunker.contextualize(chunk):\\n{f'{enriched_text[:300]}…'!r}\")\n",
    "\n",
    "        row = {\n",
    "            'content': chunk,\n",
    "            'contextualize_content' : enriched_text,\n",
    "            'url': reference_url,  # ou sérialisez selon vos besoins\n",
    "            # Ajoutez d'autres champs selon votre structure\n",
    "        }\n",
    "    \n",
    "\n",
    "    \n",
    "    data_docling.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fae5e0f-1585-4e35-a2f9-6648780cf65a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1756113064232}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_spark_docling = spark.createDataFrame(pd.DataFrame(data_docling).astype({'content': 'string'}))\n",
    "display(df_spark_docling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbd2e4dd-083f-4204-be4a-19e5bb825065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The delat table needs :\n",
    "- 'delta.enableChangeDataFeed' = 'true'\n",
    "- an id\n",
    "- timestamp for the date of creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b1f847-02bc-41b1-ae7c-31eeda472b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG demo;\n",
    "USE SCHEMA demo;\n",
    "\n",
    "CREATE OR REPLACE TABLE databricks_document_docling (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  content STRING,\n",
    "  contextualize_content STRING,\n",
    "  url STRING,\n",
    "  created_at TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true'\n",
    ");\n",
    "\n",
    "ALTER TABLE databricks_document_docling SET TBLPROPERTIES (\n",
    "  'delta.feature.allowColumnDefaults' = 'enabled'\n",
    ");\n",
    "\n",
    "ALTER TABLE databricks_document_docling ALTER COLUMN created_at SET DEFAULT CURRENT_TIMESTAMP();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b13e319-464f-4e68-9cc4-1e801c2894f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creation of the vectorstore database : \"demo.demo.databricks_document_docling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13167e5b-7055-42a8-bd6b-bc2816b6bcfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Définir le nom de votre table UC (catalog.schema.table)\n",
    "table_name = \"demo.demo.databricks_document_docling\"\n",
    "\n",
    "# Créer la table UC\n",
    "df_spark_docling.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4677436925806113,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "VectorStore_base_preprocessing_0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
