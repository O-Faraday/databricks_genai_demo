{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9c4bc5-7b29-4a77-a478-97b321aea7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deploy a RAG system with Mosaic AI Agent Evaluation and Lakehouse Applications\n",
    "\n",
    "In this chapter, you will build a **ecommerce Docs Assistant** to help users answer questions about Databricks, using :  \n",
    "- Vector database / index, \n",
    "- LLM endpoint\n",
    "- MLFlow for tracking, deployment\n",
    "- Lakehouse for data housing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f3239b-2e3d-46f4-80c5-756d3c908dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## In this notebook, we will build the pipeline to feed the vectorstore database.\n",
    "\n",
    "Our goal is to develop an assistant able to read the docs of databricks to answer dev questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20444d3-ca22-4083-aa01-96334bddaa08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract and preprocess of the contextual documents. \n",
    "\n",
    "This notebook will be used to create a delta table in the Unity Catalog that will be next used as vectorstore database.\n",
    "\n",
    "For this we will use two types of chunking method, \n",
    "- the first one with classic recursive chunking on a pdf source\n",
    "- the second with a docling extract + chunk.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e276c356-6066-4bb2-ac45-3963f9703075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet databricks-langchain==0.6.0 mlflow[databricks]==3.4.0  langchain==0.3.27 langchain_core==0.3.74 bs4 langchain_community markdownify docling pypdf2 pypdf\n",
    "\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c05f18af-caf8-42d6-b008-e6e10a86939e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0- Configuration\n",
    "\n",
    "In a first notebook, we have uploaded the pdfs from a github to a Unity Catalog Volume :\n",
    "\n",
    "catalog  : demo\n",
    "schema : demo\n",
    "volume : /Volumes/{catalog}/{schema}/raw_data/pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028e8b4b-7820-49a7-97c9-bbc2c864f049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalog et schema \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f9f647-95ea-4c00-ba9c-6bd4f5cd2664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"demo\"\n",
    "schema = \"demo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c59b5f-2d2f-4a30-abc6-9920d4ad490e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Source extract\n",
    "The source is a bunch of pdf from the dedicated UC volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3505f93a-17a6-40a0-b485-59ea6fef0de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PDF volume path\n",
    "path_volume = f\"/Volumes/{catalog}/{schema}/raw_data/pdf/\"\n",
    "pdf_list = dbutils.fs.ls(path_volume) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0f3549-2625-4788-9efc-b2e79c44f5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1- Create a table with classic chunks.  \n",
    "\n",
    "### Extracting Databricks documentation sitemap and pages\n",
    "\n",
    "For this demo, we will directly load a few pdf documents pages extract their contents.\n",
    "The library used is langchain that has integrated tools for each stage of the authoirng of agents.\n",
    "\n",
    "Here are the main steps:\n",
    "\n",
    "    Load each pdf from the volume\n",
    "    Convert each document to markdown versions, \n",
    "    \n",
    "\n",
    "### Splitting documentation pages into small chunks\n",
    "\n",
    "LLM models typically have a maximum input context length, and you won't be able to compute embeddings for very long texts. In addition, the longer your context length is, the longer it will take for the model to provide a response.\n",
    "\n",
    "Document preparation is key for your model to perform well, and multiple strategies exist depending on your dataset:\n",
    "\n",
    "    Split document into small chunks \n",
    "    Truncate documents to a fixed length chunk_size=1000, chunk_overlap=100 in our case\n",
    "    The chunk size depends on your content and how you'll be using it to craft your prompt. Adding multiple small doc chunks in your prompt might give different results than sending only a big one\n",
    "    Split into big chunks and ask a model to summarize each chunk as a one-off job, for faster live inference\n",
    "    Create multiple agents to evaluate each bigger document in parallel, and ask a final agent to craft your answer...\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8509b6-a2f9-42b2-8cea-790601554f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain_community.document_transformers import MarkdownifyTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "#md = MarkdownifyTransformer([\"script\", \"style\", \"nav\", \"footer\"])\n",
    "\n",
    "def custom_pdf_splitter(pdf_path, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "        for the url_doc, fetch the HTML content, parse it with BeautifulSoup, and split it into smaller chunks.\n",
    "        chunk_size and chunk_overlap are parameters of RecursiveCharacterTextSplitter\n",
    "    \"\"\"\n",
    "    # Load the pdf doc\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "    # Create a list of LangChain documents\n",
    "    documents =  loader.load()\n",
    "\n",
    "\n",
    "    # Apply text splitter, you can add a separator if needed\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "def create_documents(path_volume) : \n",
    "    \"\"\"\n",
    "        for each pdf, add the Documents results from custom_html_splitter\n",
    "        to the list of documents extracted form the html docs. \n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for pdf_infos in dbutils.fs.ls(path_volume) :\n",
    "        documents += custom_pdf_splitter(f\"{path_volume}{pdf_infos.name}\", chunk_size=1000)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = create_documents(path_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28aadfdd-7529-4b92-8216-9f20b3b4e3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(documents[0].metadata)\n",
    "print(documents[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1029e50-47d0-4ebf-81f5-32b5de6679b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f6c97e-3fd7-4c28-b4c3-ba7b05599f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Convert the list of Chunks split with lanchain features to columns of text, doc.content, page_label , source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2aa5526-1f57-4075-99f9-023bf1e01ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# From the list of langchain chunks create data for the vectorstore table. \n",
    "data = [{\n",
    "            'content': doc.page_content,\n",
    "            'source': str(doc.metadata['source']), \n",
    "            'page': str(doc.metadata['page_label']), \n",
    "            # Add as many information as you need\n",
    "        }\n",
    "        for doc in documents ]\n",
    "\n",
    "# Create a table in UC that can be used a vector store\n",
    "df_pandas = pd.DataFrame(data)\n",
    "df_spark = spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96cee1e-df9c-4b84-9f02-1ef92b1545a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761924041429}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95bcc27d-9d4b-4db9-8e9d-ff8f0c2e0878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a delta table with the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bc33ab-8af1-4f87-94a0-8fefb1a3e08f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG demo;\n",
    "USE SCHEMA demo;\n",
    "\n",
    "CREATE OR REPLACE TABLE pdf_document_raw (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  content STRING,\n",
    "  source STRING,\n",
    "  page STRING,\n",
    "  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.feature.allowColumnDefaults' = 'enabled'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f98bcd8-b61d-4cfc-9f55-1c7cb3dc8ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "delta.feature.allowColumnDefaults property is actually enabled by default in newer Databricks runtimes (DBR 10.4 and above), so you might not even need to explicitly set it unless you're on an older runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7e8d31-d169-4286-9505-694c0f1f21d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Définir le nom de votre table UC (catalog.schema.table)\n",
    "table_name = f\"pdf_document_raw\"\n",
    "\n",
    "# Créer la table UC\n",
    "df_spark.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9932b6-88d3-4a2b-b5f8-d7cecc81b21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2- Use the docling preparation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647ea623-ef7b-4bba-a971-0dce612074d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.document import InputDocument\n",
    "from docling.chunking import HybridChunker, HierarchicalChunker\n",
    "\n",
    "doc_convert = DocumentConverter()\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "# Docling Parse without EasyOCR\n",
    "# -------------------------\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = False\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "       InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)     \n",
    "    }\n",
    ")\n",
    "\n",
    "def docling_pdf_splitter(input_doc_path, chunk_size=500, chunk_overlap=100):\n",
    "    print(f\"Processing {input_doc_path}\")\n",
    "    start_time = time.time()\n",
    "    doc_result = doc_converter.convert(input_doc_path).document\n",
    "    end_time = time.time() - start_time\n",
    "    #Evaluation of the needed time\n",
    "    print(f\"docling_pdf_splitter took {end_time} seconds to process {input_doc_path}\")\n",
    "\n",
    "\n",
    "    print(f\"doc_result {doc_result}\")\n",
    "    chunk_iter = chunker.chunk(dl_doc=doc_result) #, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    data_docling = []\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"=== {i} ===\")\n",
    "        #print(f\"chunk.text:\\n{f'{chunk.text[:300]}…'!r}\")\n",
    "\n",
    "        enriched_text = chunker.contextualize(chunk=chunk)\n",
    "        print(f\"chunker.contextualize(chunk):\\n{f'{enriched_text[:300]}…'!r}\")\n",
    "\n",
    "        row = {\n",
    "            #'content': chunk,\n",
    "            'content' : enriched_text,\n",
    "            'source': input_doc_path,  \n",
    "            #'page': str(doc_result.metadata['page_label'])\n",
    "        }\n",
    "       \n",
    "        data_docling.append(row)\n",
    "\n",
    "    return data_docling\n",
    "\n",
    "def create_docling_documents(path_volume) : \n",
    "    \"\"\"\n",
    "        for each pdf, add the Documents results from custom_html_splitter\n",
    "        to the list of documents extracted form the html docs. \n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for pdf_infos in dbutils.fs.ls(path_volume) :\n",
    "        print(f\"Processing {pdf_infos.name}\")\n",
    "        documents += docling_pdf_splitter(f\"{path_volume}{pdf_infos.name}\", chunk_size=1000)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "\n",
    "docling_documents = create_docling_documents(path_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a057d788-5e60-4c07-b165-342c365983e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(docling_documents))\n",
    "#print(type(docling_documents[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fae5e0f-1585-4e35-a2f9-6648780cf65a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1756113064232}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True)\n",
    "])\n",
    "df_spark_docling = spark.createDataFrame(docling_documents, schema=schema)\n",
    "display(df_spark_docling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbd2e4dd-083f-4204-be4a-19e5bb825065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The delat table needs :\n",
    "- 'delta.enableChangeDataFeed' = 'true'\n",
    "- an id\n",
    "- timestamp for the date of creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7006e012-9f57-412b-add9-6948442ded11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Docling :\n",
    "Docling simplifies document processing, parsing diverse formats — including advanced PDF understanding — and providing seamless integrations with the gen AI ecosystem.\n",
    "\n",
    "It's an open-source project from IBM.\n",
    "\n",
    "This tool benefits from advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more.\n",
    "\n",
    "Ready for image integration with GraniteDocling that Supports several Visual Language Models.\n",
    "\n",
    "This at the time one of the most deployed repository from Github platform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b1f847-02bc-41b1-ae7c-31eeda472b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG demo;\n",
    "USE SCHEMA demo;\n",
    "\n",
    "CREATE OR REPLACE TABLE pdf_document_docling (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  content STRING,\n",
    "  source STRING,\n",
    "  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.feature.allowColumnDefaults' = 'enabled'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b13e319-464f-4e68-9cc4-1e801c2894f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creation of the vectorstore database : \"demo.demo.pdf_document_docling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13167e5b-7055-42a8-bd6b-bc2816b6bcfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Définir le nom de votre table UC (catalog.schema.table)\n",
    "table_name = \"pdf_document_docling\"\n",
    "\n",
    "# Créer la table UC\n",
    "df_spark_docling.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1463160803915389,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "VectorStore_pdf_preprocessing_0",
   "widgets": {
    "catalog": {
     "currentValue": "demo",
     "nuid": "0b2d4d90-a650-40b1-af65-905a76903318",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "demo",
     "nuid": "4a909bd5-6bc8-41a0-b30f-a0376fcc6366",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
