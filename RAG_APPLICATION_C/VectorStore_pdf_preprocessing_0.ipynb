{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9c4bc5-7b29-4a77-a478-97b321aea7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deploy a RAG system with Mosaic AI Agent Evaluation and Lakehouse Applications\n",
    "\n",
    "In this chapter, you will build a **Databricks Docs Assistant** to help users answer questions about Databricks, using :  \n",
    "- Vector database / index, \n",
    "- LLM endpoint\n",
    "- MLFlow for tracking, deployment\n",
    "- Lakehouse for data housing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f3239b-2e3d-46f4-80c5-756d3c908dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## In this notebook, we will build the pipeline to feed the vectorstore database.\n",
    "\n",
    "Our goal is to develop an assistant able to read the Databricks docs to answer dev questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20444d3-ca22-4083-aa01-96334bddaa08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extraction and preprocessing of the contextual documents. \n",
    "\n",
    "This notebook will be used to create a delta table in the Unity Catalog that will be next used as vectorstore database.\n",
    "\n",
    "For this we will use two types of chunking method, \n",
    "- the first one with classic recursive chunking on a pdf source\n",
    "- the second with a docling extract + chunk.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e276c356-6066-4bb2-ac45-3963f9703075",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet databricks-langchain==0.6.0 mlflow[databricks]==3.4.0  langchain==0.3.27 langchain_core==0.3.74 bs4 langchain_community markdownify docling pypdf2 pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e02955d-cf64-4142-81c6-0315fda5115e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05f18af-caf8-42d6-b008-e6e10a86939e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0- Configuration\n",
    "\n",
    "In a first notebook, we have uploaded the PDFs from a GitHub repository to a Unity Catalog Volume :\n",
    "\n",
    "catalog  : demo\n",
    "schema : demo\n",
    "volume : /Volumes/{catalog}/{schema}/raw_data/pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028e8b4b-7820-49a7-97c9-bbc2c864f049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalog and schema \n",
    "The magic %run command will load the UC catalog config and navigate to our \"demo\" work schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f9f647-95ea-4c00-ba9c-6bd4f5cd2664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../_config/config_unity_catalog\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c59b5f-2d2f-4a30-abc6-9920d4ad490e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Source extract\n",
    "The source is a collection of PDFs from the dedicated UC volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3505f93a-17a6-40a0-b485-59ea6fef0de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PDF volume path\n",
    "path_volume = f\"/Volumes/{catalog}/{schema}/raw_data/pdf/\"\n",
    "pdf_list = dbutils.fs.ls(path_volume)\n",
    "print(\"PDF list : \") \n",
    "for pdf_infos in pdf_list[:2] : \n",
    "    print(pdf_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0f3549-2625-4788-9efc-b2e79c44f5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1- Create a table with classic chunks.  \n",
    "\n",
    "### Extracting Databricks documentation sitemap and pages\n",
    "\n",
    "For this demo, we will directly load a few PDF documents pages and extract their contents.\n",
    "The library used is Langchain that has integrated tools for each stage of the authoring of agents.\n",
    "\n",
    "Here are the main steps:\n",
    "\n",
    "- Extract URLs from the Databricks docs sitemap\n",
    "- Download each page\n",
    "- Split them into small chunks for our vector search to be able to digest them accurately and add them to our index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d5e6e9-c60e-45f5-84ed-1e1f4ce02db0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Classic method functions with recursive splitter and loaders, "
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import time\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "def classic_pdf_splitter(pdf_path) : \n",
    "    \"\"\"\n",
    "    Load and split a PDF using PyPDFLoader and RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "    \n",
    "    Returns:\n",
    "        List of Document chunks with metadata\n",
    "    \"\"\"\n",
    "    print(f\"Processing {pdf_path}\")\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    #Return the chunked pages\n",
    "    return text_splitter.split_documents(pages)\n",
    "\n",
    "def create_classic_documents(path_volume) : \n",
    "    \"\"\"\n",
    "    For each PDF, add the Document results from classic_pdf_splitter\n",
    "    to the list of documents extracted from the sources. \n",
    "    \n",
    "    Args:\n",
    "        path_volume: Path to the volume containing PDF files\n",
    "    \n",
    "    Returns:\n",
    "        List of all document chunks from all PDFs\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    start_time = time.time()\n",
    "    for pdf_infos in dbutils.fs.ls(path_volume) :\n",
    "        print(f\"Processing {pdf_infos.name}\")\n",
    "        documents += classic_pdf_splitter(f\"{path_volume}{pdf_infos.name}\")\n",
    "    \n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # Evaluation of the processing time\n",
    "    print(f\"classic_pdf_splitter took {end_time} seconds to process classic documents\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = create_classic_documents(path_volume)\n",
    "print(f\"{len(documents)} chunks ! \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac59b8d8-fa4a-4b12-98a5-2b9f3a3bae8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pathlib import Path\n",
    "schema = StructType([\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"source_name\", StringType(), True),\n",
    "])\n",
    "doc_dict = [{'content' : doc.page_content,\n",
    "              'source': doc.metadata['source'],\n",
    "              'source_name' : Path(doc.metadata['source']).name,\n",
    "              } for doc in documents]\n",
    "\n",
    "df_spark = spark.createDataFrame(doc_dict, schema=schema)\n",
    "display(df_spark.limit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7869da-2b3e-46e0-a64e-dc11f04f6ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The delta table needs :\n",
    "- 'delta.enableChangeDataFeed' = 'true'\n",
    "- an id\n",
    "- timestamp for the date of creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ae7ab8-e1a7-4c73-be0d-fd0f4d4e0f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE pdf_document (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "  content STRING,\n",
    "  source STRING,\n",
    "  source_name STRING,\n",
    "  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.feature.allowColumnDefaults' = 'enabled'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51b4a2e-03c0-44ab-b1c0-29bdaad84e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creation of the vectorstore database : \"demo.demo.pdf_document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd02c54-d7be-461e-8bb0-88df63cffb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the name of your UC table (catalog.schema.table)\n",
    "table_name = \"pdf_document\"\n",
    "\n",
    "# Create the UC table\n",
    "df_spark.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d2c1fb-1af5-493e-95e0-fb86ee7bd0f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check that the table exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf95b94-a09e-4c1a-b3a8-e0b43be31b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM pdf_document limit 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d533f78-72d1-4a78-a5db-24c0c18d1631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2- Create a table with the second method : Docling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3fa086-da66-418f-ad81-e3f3d86d6a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "import time\n",
    "\n",
    "doc_converter = DocumentConverter()\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    max_tokens=512,\n",
    "    include_section_info=True,\n",
    ")\n",
    "\n",
    "def docling_pdf_splitter(pdf_path) : \n",
    "    \"\"\"\n",
    "    Load and split a PDF using Docling's HybridChunker.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing chunk content and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Processing {pdf_path}\")\n",
    "\n",
    "    doc_result = doc_converter.convert(pdf_path).document\n",
    "    print(f\"doc_result {doc_result.name}\")\n",
    "    chunk_iter = chunker.chunk(dl_doc=doc_result)\n",
    "    data_docling = []\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        enriched_text = chunker.contextualize(chunk=chunk)\n",
    "        row = {\n",
    "            #'content': chunk,\n",
    "            'content' : enriched_text,\n",
    "            'source': pdf_path, \n",
    "            'source_name' : Path(pdf_path).name, \n",
    "        }\n",
    "        data_docling.append(row)\n",
    "    print(f\"=> {i} Chunks \")\n",
    "    return data_docling\n",
    "\n",
    "def create_docling_documents(path_volume) : \n",
    "    \"\"\"\n",
    "    For each PDF, add the Document results from docling_pdf_splitter\n",
    "    to the list of documents extracted from the sources. \n",
    "    \n",
    "    Args:\n",
    "        path_volume: Path to the volume containing PDF files\n",
    "    \n",
    "    Returns:\n",
    "        List of all document chunks from all PDFs\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    start_time = time.time()\n",
    "    for pdf_infos in dbutils.fs.ls(path_volume) :\n",
    "        print(f\"Processing {pdf_infos.name}\")\n",
    "        documents += docling_pdf_splitter(f\"{path_volume}{pdf_infos.name}\")\n",
    "    \n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # Evaluation of the processing time\n",
    "    print(f\"docling_pdf_splitter took {end_time} seconds to process docling documents\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "docling_documents = create_docling_documents(path_volume)\n",
    "print(f\"{len(docling_documents)} chunks ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fae5e0f-1585-4e35-a2f9-6648780cf65a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1756113064232}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"source_name\", StringType(), True),\n",
    "])\n",
    "df_spark_docling = spark.createDataFrame(docling_documents, schema=schema)\n",
    "display(df_spark_docling.limit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd2e4dd-083f-4204-be4a-19e5bb825065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The delta table needs :\n",
    "- 'delta.enableChangeDataFeed' = 'true'\n",
    "- an id\n",
    "- timestamp for the date of creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7006e012-9f57-412b-add9-6948442ded11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Docling :\n",
    "Docling simplifies document processing, parsing diverse formats — including advanced PDF understanding — and providing seamless integrations with the gen AI ecosystem.\n",
    "\n",
    "It's an open-source project from IBM.\n",
    "\n",
    "This tool benefits from advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more.\n",
    "\n",
    "Ready for image integration with GraniteDocling that Supports several Visual Language Models.\n",
    "\n",
    "This is currently one of the most deployed repositories on the GitHub platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b1f847-02bc-41b1-ae7c-31eeda472b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE pdf_document_docling (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "  content STRING,\n",
    "  source STRING,\n",
    "  source_name STRING,\n",
    "  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.feature.allowColumnDefaults' = 'enabled'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b13e319-464f-4e68-9cc4-1e801c2894f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creation of the vectorstore database : \"demo.demo.pdf_document_docling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13167e5b-7055-42a8-bd6b-bc2816b6bcfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the name of your UC table (catalog.schema.table)\n",
    "table_name = \"pdf_document_docling\"\n",
    "\n",
    "# Create the UC table\n",
    "df_spark.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128ff1f4-b90b-4585-b627-573055d8b2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check that the table exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89402ef3-d320-4cd4-a823-3d6946482927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM pdf_document_docling limit 2;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8729955896682447,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "VectorStore_pdf_preprocessing_0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
