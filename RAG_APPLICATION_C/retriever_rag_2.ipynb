{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791e5301-4792-4e48-b3fb-3a442887e3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retriever notebook :\n",
    "\n",
    "In this notebook, we will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daed9ca7-2750-44e4-b41d-6e24126ad491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-sdk==0.64.0 \"databricks-langchain>=0.4.0\"  \"mlflow[databricks]==3.4.0\" langchain==0.3.25 langchain_core==0.3.59 databricks-vectorsearch==0.57 pydantic==2.10.1 zeroentropy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e44a24e-2fcd-439b-a3d0-3c40a7bfb098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "source du dataset\n",
    "https://github.com/mlflow/mlflow/blob/master/examples/llms/RAG/question_answer_source.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c23881-5bba-4de7-8af1-ec5e81484fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0- Init of the mlflow tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ada3694-ad76-45d5-9e16-e35de5170c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this cell, the mlflow experiment is set to \"rag_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da652e4-0974-4015-9861-be832fccaa7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../_config/config_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a32503-343f-49fc-adaa-adc82990cb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1- Create eval dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa423f5-71d2-43bc-9cd2-6a20e5f3c613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In  this cell, some questions are extracted from the mlflow repository on github  to create an evaluation dataset. \n",
    "\n",
    "\"https://raw.githubusercontent.com/mlflow/mlflow/master/examples/llms/RAG/static_evaluation_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56631ef2-af3a-4fd3-b15b-929099e2bcb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "QUESTIONS_SOURCE = \"https://raw.githubusercontent.com/mlflow/mlflow/master/examples/llms/RAG/static_evaluation_dataset.csv\"\n",
    "eval_data = pd.read_csv(QUESTIONS_SOURCE)\n",
    "question_serie = spark.createDataFrame(eval_data[\"question\"]).withColumnRenamed(\"_1\", \"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8800953d-5395-4f04-9584-6630d648dd46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the work schema from your UC demo.demo, a delta table is created to log the questions.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd9379b-24a4-437c-be20-8fea38dd2629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Catalog name\n",
    "dbutils.widgets.text(\"catalog\", \"demo\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "# Cellule PySpark\n",
    "dbutils.widgets.text(\"schema\", \"demo\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "# table name\n",
    "dbutils.widgets.text(\"table_name\", \"databricks_mlflow_questions\")\n",
    "table_name = dbutils.widgets.get(\"table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782d7dea-88e6-4911-a125-dbebd862e942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG IDENTIFIER(:catalog);\n",
    "USE SCHEMA IDENTIFIER(:schema);\n",
    "\n",
    "CREATE OR REPLACE TABLE  IDENTIFIER(:table_name) (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  question STRING,\n",
    "  created_at TIMESTAMP DEFAULT current_timestamp\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.feature.allowColumnDefaults' = 'enabled'\n",
    ");\n",
    "\n",
    "--ALTER TABLE databricks_document_docling ALTER COLUMN created_at SET DEFAULT CURRENT_TIMESTAMP();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb4867c-3e37-471b-8054-ba74163c81e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_full_name = f\"{catalog}.{schema}.{table_name}\"\n",
    "\n",
    "# Cr√©er la table UC\n",
    "question_serie.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(table_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1265bdc-68fc-4394-b559-04c2dbf2153c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT * FROM IDENTIFIER(:table_name) limit 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e569a29-bf23-42f8-8ec1-d8b8970f4bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2- Create rag chain\n",
    "\n",
    "No serving endpoints. The system is allready full of serving endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1869171f-7398-43ca-8724-e45a6883f2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "rag_chain_config = {\n",
    "    \"databricks_resources\": {\n",
    "        \"llm_endpoint_name\": \"chat_gpt_4o_mini\",\n",
    "        \"vector_search_endpoint_name\": \"databricks_document_vs_endpoint_docling\",\n",
    "    },\n",
    "    \"input_example\": {\n",
    "        \"messages\": [{\"content\": \"What are  the differences between MLflow, managed and open source, on databricks ?\", \"role\": \"user\"}]\n",
    "    },\n",
    "    \"llm_config\": {\n",
    "        \"llm_parameters\": {\"max_tokens\": 1500, \"temperature\": 0.01},\n",
    "        \"llm_prompt_template\": \"You are a trusted AI assistant that helps answer questions based only on the provided information. If you do not know the answer to a question, you truthfully say you do not know. Here is the history of the current conversation you are having with your user: {chat_history}. And here is some context which may or may not help you answer the following question: {context}.  Answer directly, do not repeat the question, do not start with something like: the answer to the question, do not add AI in front of your answer, do not say: here is the answer, do not mention the context or the question. Based on this context, answer this question: {question}\",\n",
    "        \"llm_prompt_template_variables\": [\"context\", \"chat_history\", \"question\"],\n",
    "    },\n",
    "    \"retriever_config\": {\n",
    "        \"chunk_template\": \"Passage: {chunk_text}\\n\",\n",
    "        \"data_pipeline_tag\": \"poc\",\n",
    "        \"parameters\": {\"k\": 3, \"query_type\": \"ann\"},\n",
    "        \"schema\": {\"raw_text\": \"content\", \"chunk_text\": \"contextualize_content\", \"document_uri\": \"url\", \"primary_key\": \"id\"},\n",
    "        \"vector_search_index\": f\"{catalog}.{schema}.databricks_document_docling_vs_index\",\n",
    "        \"source_table\":f\"{catalog}.{schema}.databricks_document_docling\"\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    with open('rag_chain_config.yaml', 'w') as f:\n",
    "        yaml.dump(rag_chain_config, f)\n",
    "except:\n",
    "    print('pass to work on build job')\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d9e187-5eb8-4e4b-9703-4476b045094e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile rag_chain.py\n",
    "import os\n",
    "import mlflow\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from databricks_langchain import DatabricksVectorSearch, ChatDatabricks\n",
    "from datetime import datetime\n",
    "\n",
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')\n",
    "#Create specific config\n",
    "databricks_resources = model_config.get(\"databricks_resources\")\n",
    "\n",
    "llm_config = model_config.get(\"llm_config\")\n",
    "\n",
    "retriever_config = model_config.get(\"retriever_config\")\n",
    "\n",
    "vector_search_schema = retriever_config.get(\"schema\")\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "#@mlflow.trace(name=\"extract_previous_messages\")\n",
    "def extract_previous_messages(chat_messages_array):\n",
    "    messages = \"\\n\"\n",
    "    for msg in chat_messages_array[:-1]:\n",
    "        messages += (msg[\"role\"] + \": \" + msg[\"content\"] + \"\\n\")\n",
    "    return messages\n",
    "\n",
    "#@mlflow.trace(name=\"combine_all_messages_for_vector_search\")\n",
    "def combine_all_messages_for_vector_search(chat_messages_array):\n",
    "    return extract_previous_messages(chat_messages_array) + extract_user_query_string(chat_messages_array)\n",
    "\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    endpoint=databricks_resources.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=retriever_config.get(\"vector_search_index\"),\n",
    "    columns=[\n",
    "        vector_search_schema.get(\"primary_key\"),\n",
    "        vector_search_schema.get(\"chunk_text\"),\n",
    "        vector_search_schema.get(\"document_uri\"),\n",
    "    ],\n",
    ").as_retriever(search_kwargs=retriever_config.get(\"parameters\"))\n",
    "\n",
    "@mlflow.trace(name=\"vector_search_retrieval\", span_type=\"RETRIEVER\", attributes={\n",
    "    \"index_name\": retriever_config.get(\"vector_search_index\"),\n",
    "    \"endpoint\": databricks_resources.get(\"vector_search_endpoint_name\")\n",
    "})\n",
    "def retrieve_documents(query: str):\n",
    "    \"\"\"Wrapper pour tracks the retriever calls\"\"\"\n",
    "    results = vector_search_as_retriever.invoke(query)\n",
    "    # Access current span to add attrbutes\n",
    "    current_span = mlflow.get_current_active_span()\n",
    "    \n",
    "    results = vector_search_as_retriever.invoke(query)\n",
    "    \n",
    "    # Add num results attirbutes\n",
    "    if current_span:\n",
    "        current_span.set_attribute(\"num_results\", len(results))\n",
    "    return results\n",
    "\n",
    "# Required to:\n",
    "# 1. Enable the RAG Studio Review App to properly display retrieved chunks\n",
    "# 2. Enable evaluation suite to measure the retriever\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=vector_search_schema.get(\"primary_key\"),\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    doc_uri=vector_search_schema.get(\"document_uri\")\n",
    ")\n",
    "\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt\n",
    "#@mlflow.trace(name=\"format_context\")\n",
    "def format_context(docs):\n",
    "    #chunk_template = retriever_config.get(\"chunk_template\")\n",
    "    chunk_contents = [\n",
    "        f\"Document : {d.page_content}\"\n",
    "        for d in docs\n",
    "    ]\n",
    "    #print(f\"chunk_contents : {chunk_contents}\")\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "\n",
    "# Prompt Template for generation\n",
    "prompt = PromptTemplate(\n",
    "    template=llm_config.get(\"llm_prompt_template\"),\n",
    "    input_variables=llm_config.get(\"llm_prompt_template_variables\"),\n",
    ")\n",
    "\n",
    "# FM for generation\n",
    "model = ChatDatabricks(\n",
    "    endpoint=databricks_resources.get(\"llm_endpoint_name\"),\n",
    "    extra_params=llm_config.get(\"llm_parameters\"),\n",
    ")\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    { # Inputs prompt variables\n",
    "        \"question\": # user question extracted for the list of messages  \n",
    "            itemgetter(\"messages\") |  #gathers the list of messages\n",
    "            RunnableLambda(extract_user_query_string), # extracts the last message user query\n",
    "        \"context\": # pipe the retriever\n",
    "            itemgetter(\"messages\") | \n",
    "            RunnableLambda(combine_all_messages_for_vector_search)| #Prepare the input for the vector search \n",
    "            retrieve_documents | # Retrieve the pertinent context\n",
    "            RunnableLambda(format_context), \n",
    "        \"chat_history\": # message history\n",
    "            itemgetter(\"messages\") | \n",
    "            RunnableLambda(extract_previous_messages) # Extracts the messages history\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#print(chain.invoke(model_config.get(\"input_example\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03988c0-b91c-4ff9-bb19-750fbe6989d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.resources import DatabricksVectorSearchIndex, DatabricksServingEndpoint\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"demo_rag_chain\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=\"rag_chain.py\",  # Chain code file e.g., /path/to/the/chain.py \n",
    "        model_config=rag_chain_config,  # Chain configuration \n",
    "        artifact_path=\"rag_chain\",  # Required by MLflow\n",
    "        input_example=model_config.get(\"input_example\"),  # Save the chain's input schema\n",
    "        resources=[\n",
    "            DatabricksVectorSearchIndex(index_name=model_config.get(\"retriever_config\").get(\"vector_search_index\")),\n",
    "            DatabricksServingEndpoint(endpoint_name=model_config.get(\"databricks_resources\").get(\"llm_endpoint_name\"))\n",
    "        ],\n",
    "        extra_pip_requirements=[\"databricks-connect\"]\n",
    "    )\n",
    "\n",
    "# Test the chain locally\n",
    "rag_chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "rag_chain.invoke(model_config.get(\"input_example\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4896179805395418,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "retriever_rag_2",
   "widgets": {
    "catalog": {
     "currentValue": "demo",
     "nuid": "2e9f3c2e-6a3c-4d27-9e92-f0fcad83de19",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "demo",
     "nuid": "ab0428c8-540f-4ba0-afa0-2bf4f07924b9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "databricks_mlflow_questions",
     "nuid": "a9220d93-3ea0-490a-8e92-07e2d84dcf96",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_mlflow_questions",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks_mlflow_questions",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
