{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e05fe50-e8b8-48d6-b8c6-1ac9f77a75a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## In this notebook, you will learn how to create model serving endpoints that deploy and serve foundation models.\n",
    "\n",
    "Mosaic AI Model Serving supports the following models:\n",
    "\n",
    "- External models. These are foundation models that are hosted outside of Databricks. Endpoints that serve external models can be centrally governed and customers can establish rate limits and access control for them. Examples include foundation models like OpenAI's GPT family and Anthropic's Claude.\n",
    "\n",
    "- State-of-the-art open foundation models made available by Foundation Model APIs. These models are curated foundation model architectures that support optimized inference. Base models, like Meta-Llama-3.1-70B-Instruct, GTE-Large, and Mistral-7B are available for immediate use with pay-per-token pricing. Production workloads, using base or fine-tuned models, can be deployed with performance guarantees using provisioned throughput.\n",
    "\n",
    "Model Serving provides the following options for model serving endpoint creation:\n",
    "\n",
    "- The Serving UI\n",
    "- REST API\n",
    "- MLflow Deployments SDK\n",
    "\n",
    "For creating endpoints that serve traditional ML or Python models, see Create custom model serving endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31061070-edc9-48cf-a7e6-a38ee499a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a foundation model serving endpoint\n",
    "\n",
    "You can create an endpoint that serves fine-tuned variants of foundation models made available using Foundation Model APIs provisioned throughput. See Create your provisioned throughput endpoint using the REST API.\n",
    "\n",
    "For foundation models that are made available using Foundation Model APIs pay-per-token, Databricks automatically provides specific endpoints to access the supported models in your Databricks workspace. To access them, select the Serving tab in the left sidebar of the workspace. The Foundation Model APIs are located at the top of the Endpoints list view.\n",
    "\n",
    "For querying these endpoints, see Use foundation models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ef577e-3af6-4d74-816a-21d1ce6684b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1 configuration of the model endpoints\n",
    "\n",
    "In this demo we will define 2 models, \n",
    "- The chat model gpt_4o_mini, quite small , will be enough for the demos.\n",
    "- The embedding model, dedicated to the retriever component. It will be used for the meaning proximity between sentences or documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "196d7aca-0c02-4e89-9dee-07fd4dd784a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models_config = [\n",
    "    {               # Here is the first LLM, it's an openai model \n",
    "        \"name\":\"chat_gpt_4o_mini\", # Name of the endpoint that will be created\n",
    "        \"config\": {\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"chat\",\n",
    "                    \"external_model\": { # openai is a commercial provider, so you will use the api for each submitted request\n",
    "                        \"name\": \"gpt-4o-mini\", # name of the model selected, here, the size matters so mini  is a good choice for a demo\n",
    "                        \"provider\": \"openai\", # provider; the provider is the incontournable openai\n",
    "                        \"task\": \"llm/v1/chat\",  # Category of the task, here, it's a llm, version 1 dedicated to chat.\n",
    "                        \"openai_config\": {\n",
    "                            \"openai_api_key\": \"{{secrets/llm_secrets/openai_api_key}}\", # You must have already registered the key in the llm_secret store\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {               # embeddings text-embedding-3-small\n",
    "        \"name\":\"text_embedding_3_large\",\n",
    "        \"config\": {\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"embeddings\",\n",
    "                    \"external_model\": {\n",
    "                        \"name\": \"text-embedding-3-large\",\n",
    "                        \"provider\": \"openai\",\n",
    "                        \"task\": \"llm/v1/embeddings\", # Category of the task, here, it's a llm, version 1 dedicated to embedding.\n",
    "                        \"openai_config\": {\n",
    "                            \"openai_api_key\": \"{{secrets/llm_secrets/openai_api_key}}\",\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb74414-dd6d-466e-93c2-2f2c8299d9ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2 Deploy the endpoints\n",
    "\n",
    "After deployed the agent will be available through endpoints.\n",
    "\n",
    "The creation can take a few minutes, the state will give information upon the readyness of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb55ba7a-fbe6-4224-bf03-50e452f9e8f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "for config in models_config :\n",
    "    try:\n",
    "        \n",
    "        endpoint = client.create_endpoint(**config)\n",
    "    \n",
    "        print(\"Endpoint created !\")\n",
    "        print(f\"Name: {endpoint.get('name')}\")\n",
    "        print(f\"ID: {endpoint.get('id')}\")\n",
    "        print(f\"State: {endpoint.get('state')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while creating the endpoint: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e22b11b5-e856-4fa1-809b-a5d0fa27c177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3 Serving\n",
    "On the serving menu, you have access to your model endpoint library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67b8bb0c-640a-4140-93fd-0e72edd4f07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This last cell can be used if don't need a model anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62937866-2c4e-47e5-902d-92891fee27dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#client.delete_endpoint(\"chat_gpt_4o_mini\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Create_endpoints",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
