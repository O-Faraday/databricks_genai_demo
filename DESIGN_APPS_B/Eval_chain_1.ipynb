{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd487d7-c6cf-4fc9-b704-ab8971faa4c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "%md\n",
    "# Simple chain Evaluation\n",
    "\n",
    "After creating the LLM chain called \"simple_chain\". It time to evaluate the model.\n",
    "\n",
    "In this notebook, we will explore the metrics and MLflow help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db5ed226-6a5b-43d5-b1ce-75ffdeea7e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet databricks-langchain==0.6.0 mlflow[databricks]==3.4.0  langchain==0.3.27 langchain_core==0.3.74 textstat\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50192b1-ffad-45ba-83f5-58b0080d5c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load the mlflow config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21552a6d-1a4d-46cf-9c7f-ac91baeba6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../_config/config_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0c557a-b180-4191-8031-cf1c388ec107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0- Call the model endpoint\n",
    "\n",
    "We are using the model endpoint of the monolithic LLM we've registered, simple_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba506d9-58e4-4d28-8d5a-35832624657e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.deployments\n",
    "from typing import List, Dict\n",
    "\n",
    "# Launch  a deploy client \n",
    "mlflow_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "endpoint_name = \"simple_chain\"\n",
    "# A few questions about the databricks mlflow system\n",
    "DOCS_DATA = [\n",
    "    {\"question\": \"what are mlflow versions supported on databricks ?\"},\n",
    "    {\"question\": \"what is a langchain mlflow flavor?\"},\n",
    "    {\"question\": \"what is a scorer mlflow ?\"},\n",
    "    {\"question\": \"how do I register a model ?\"}\n",
    "]\n",
    "\n",
    "\n",
    "@mlflow.trace\n",
    "def generate_docs(question: str) -> dict:\n",
    "    \"\"\" the simple_chain model to predict.\"\"\"\n",
    "\n",
    "    response = mlflow_client.predict(\n",
    "        endpoint=\"simple_chain\", # The LLM chain that was created\n",
    "        inputs={\"inputs\": {\"question\": question}} # Some questions you want answers\n",
    "    )\n",
    "\n",
    "    return {\"response\": response[\"predictions\"][0]}\n",
    "\n",
    "# Test the application\n",
    "for q in DOCS_DATA : \n",
    "    print(\"response : \", generate_docs(q[\"question\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a458539d-b21e-42e2-855b-748216fb68ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1- Creation of an evaluation dataset\n",
    "\n",
    "Extraction from MLflow experiments the data to create an evalution dataset.\n",
    "\n",
    "### What are Evaluation Datasets?\n",
    "\n",
    "Evaluation Datasets in MLflow provide a structured way to organize and manage test data for GenAI applications. They serve as centralized repositories for test inputs, expected outputs (expectations), and evaluation results, enabling systematic quality assessment across your AI development lifecycle.\n",
    "\n",
    "Evaluation datasets bridge the gap between ad-hoc testing and systematic quality assurance, providing the foundation for reproducible evaluations, regression testing, and continuous improvement of your GenAI applications.\n",
    "\n",
    "We ll go through the last 10 minutes traces to build the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e8bd99-601f-47d6-9079-0073f5971e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ebadc07-5dbd-4196-aa88-23d54c13e3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.datasets import (\n",
    "    create_dataset,\n",
    "    get_dataset\n",
    ")\n",
    "import time\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "# 1. Create an evaluation dataset\n",
    "\n",
    "# Replace with a Unity Catalog schema where you have CREATE TABLE permission\n",
    "catalog_name = \"demo\"\n",
    "schema_name = \"demo\"\n",
    "uc_schema = f\"{catalog_name}.{catalog_name}\"\n",
    "# This table will be created in the above UC schema\n",
    "evaluation_dataset_table_name = \"extraction_infos_doc_mlflow_eval\"\n",
    "eval_dataset = create_dataset(\n",
    "    uc_table_name=f\"{uc_schema}.{evaluation_dataset_table_name}\",\n",
    ")\n",
    "\n",
    "print(f\"Created evaluation dataset: {uc_schema}.{evaluation_dataset_table_name}\")\n",
    "\n",
    "# 2. Search for the simulated production traces from step 2: get traces from the last 10 days with our trace name.\n",
    "ten_minutes_ago = int((time.time() - 10 * 60 ) * 1000)\n",
    "\n",
    "traces = mlflow.search_traces(\n",
    "    filter_string=f\"attributes.timestamp_ms > {ten_minutes_ago} AND \"\n",
    "                 f\"attributes.status = 'OK'\",\n",
    "    order_by=[\"attributes.timestamp_ms DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(traces)} successful traces from beta test\")\n",
    "\n",
    "# 3. Add the traces to the evaluation dataset\n",
    "eval_dataset.merge_records(traces)\n",
    "print(f\"Added {len(traces)} records to evaluation dataset\")\n",
    "\n",
    "# Preview the dataset\n",
    "eval_dataset_df = eval_dataset.to_df()\n",
    "print(f\"\\nDataset preview:\")\n",
    "print(f\"Total records: {len(eval_dataset_df)}\")\n",
    "print(\"\\nSample record:\")\n",
    "sample = eval_dataset_df.iloc[0]\n",
    "print(f\"Inputs: {sample['inputs']}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccf3f1c-5892-40d1-b91b-a115ea420c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observations\n",
    "We have extracted the last traces, we've just created to build the dataset.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b928ee-534e-48de-b002-bb3aab1a2c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2- Scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6acb7c43-1b0e-4403-a360-c4288ee26eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What are Scorers?\n",
    "\n",
    "Scorers in MLflow are evaluation functions that assess the quality of your GenAI application outputs.\n",
    "\n",
    "Scorers transform subjective quality assessments into measurable metrics, enabling you to track performance, compare models, and ensure your applications meet quality standards. \n",
    "\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Automated Quality Assessment : Replace manual review processes with automated scoring that can evaluate thousands of outputs consistently and at scale.\n",
    "\n",
    "- Safety & Compliance Validation : Systematically check for harmful content, bias, PII leakage, and regulatory compliance. The role of the guardrails\n",
    "\n",
    "- A/B Testing & Model Comparison : You can build objective criteria to compare differents models, or any evolutions in the components, such as the prompt.\n",
    "\n",
    "- Continuous Quality Monitoring : In production, you can monitor your app and dectect loss in performance due to drifts for example.\n",
    "\n",
    "\n",
    "### Types of scorers : \n",
    "\n",
    "MLflow provides several types of scorers to address different evaluation needs\n",
    "\n",
    "- Code-based Scorers : deterministic scorers that can be calculated algorithmically like ROUGE scores, exact match, or custom business logic.\n",
    "- LLM-as-judges scorers : Use LLM to evaluate subjective qualities like coherence, and style. These scorers can understand context and nuance that rule-based systems miss.\n",
    "- Human-Aligned Judges : LLM judges fine-tuned with human feedback to match your specific quality standards. \n",
    "- Agent-as-a-Judge : Autonomous agents that analyze execution traces to evaluate not just outputs, but the entire process. They can assess tool usage, reasoning chains, and error handling. (Not present here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2764a30d-7d04-4630-b1b8-7f61426c3e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1-1 code based scorers\n",
    "\n",
    "Create a code based scorer.\n",
    "This score evaluates the text complexity using Flesh Reading Ease.\n",
    "Score = 206.835 - 1.015 × (total_mots / total_phrases) - 84.6 × (total_syllabes / total_mots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c66695-54cf-42a0-a28a-2882437c7421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import (\n",
    "    scorer,\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    "    Guidelines,\n",
    ")\n",
    "\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType, Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f818b6e-1d2a-4f38-a6e7-13a18c65fbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "@scorer\n",
    "def reading_level(outputs: dict[str]) -> Feedback: # Code based scorer\n",
    "    \"\"\"\n",
    "        Evaluate text complexity using Flesch Reading Ease.\n",
    "        Higher scores indicate easier reading \n",
    "        \n",
    "        Args:\n",
    "            outputs (str)\n",
    "                The generated text to evaluate.\n",
    "        Returns:\n",
    "            Feedback: A feedback object with the following attributes:\n",
    "        The scorer is converted to be in 0. ->   1.0 range  \n",
    "    \"\"\"\n",
    "    score = textstat.flesch_reading_ease(outputs[\"response\"])  \n",
    "\n",
    "\n",
    "    if score >= 60:\n",
    "        level = \"easy\"\n",
    "        rationale = f\"Reading ease score of {score:.1f} - accessible to most readers\"\n",
    "    elif score >= 30:\n",
    "        level = \"moderate\"\n",
    "        rationale = f\"Reading ease score of {score:.1f} - college level complexity\"\n",
    "    else:\n",
    "        level = \"difficult\"\n",
    "        rationale = f\"Reading ease score of {score:.1f} - expert level required\"\n",
    "\n",
    "    return Feedback(value=score/ 100., rationale=rationale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc9e2cc-0982-4fcc-bc17-948c08f40bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1-2 LLM as judge\n",
    "Use your own LLM for a judge\n",
    "\n",
    "Integrate a custom or externally hosted LLM within a scorer. The scorer handles API calls, input/output formatting, and generates Feedback from your LLM's response, giving full control over the judging process.\n",
    "\n",
    "You can also set the source field in the Feedback object to indicate the source of the assessment is an LLM judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c187146-c14c-46f6-b861-bdbd17cb993a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "# Assume `generated_traces` is available from the prerequisite code block.\n",
    "# Assume `client` (OpenAI SDK client configured for Databricks) is available from the prerequisite block.\n",
    "# client = OpenAI(...)\n",
    "\n",
    "# Define the prompts for the Judge LLM.\n",
    "judge_system_prompt = \"\"\"\n",
    "You are an impartial AI assistant responsible for evaluating the quality of a response generated by another AI model.\n",
    "Your evaluation should be based on the original user query and the AI's response.\n",
    "Provide a quality score as an integer from 1 to 5 (1=Poor, 2=Fair, 3=Good, 4=Very Good, 5=Excellent).\n",
    "Also, provide a brief rationale for your score.\n",
    "\n",
    "Your output MUST be a single valid JSON object with two keys: \"score\" (an integer) and \"rationale\" (a string).\n",
    "Example:\n",
    "{\"score\": 4, \"rationale\": \"The response was mostly accurate and helpful, addressing the user's query directly.\"}\n",
    "\"\"\"\n",
    "judge_user_prompt = \"\"\"\n",
    "Please evaluate the AI's Response below based on the Original User Query.\n",
    "\n",
    "Original User Query:\n",
    "```{user_query}```\n",
    "\n",
    "AI's Response:\n",
    "```{llm_response_from_app}```\n",
    "\n",
    "Provide your evaluation strictly as a JSON object with \"score\" and \"rationale\" keys.\n",
    "\"\"\"\n",
    "\n",
    "@scorer\n",
    "def answer_quality(inputs: dict[str, Any], outputs: str) -> Feedback:\n",
    "    user_query = inputs[\"question\"]\n",
    "\n",
    "    # Call the Judge LLM using the OpenAI SDK client.\n",
    "    predictions = mlflow_client.predict(\n",
    "        endpoint=\"chat_gpt_4o_mini\",\n",
    "        inputs={\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": judge_system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": judge_user_prompt.format(\n",
    "                        user_query=user_query,\n",
    "                        llm_response_from_app=outputs[\"response\"]\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Parse the Judge LLM's JSON output.\n",
    "    judge_eval_json = json.loads(predictions[\"choices\"][0][\"message\"][\"content\"])\n",
    "    parsed_score = int(judge_eval_json[\"score\"])\n",
    "    parsed_rationale = judge_eval_json[\"rationale\"]\n",
    "\n",
    "    return Feedback(\n",
    "        value=parsed_score,\n",
    "        rationale=parsed_rationale,\n",
    "        # Set the source of the assessment to indicate the LLM judge used to generate the feedback\n",
    "        source=AssessmentSource(\n",
    "            source_type=AssessmentSourceType.LLM_JUDGE,\n",
    "            source_id=\"chat_gpt_4o_mini\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae2d195-35d5-451e-88b7-72cd756f976d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1-3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "505307a8-717a-4a72-bee7-5a9ce4df6003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Guidelines scorers\n",
    "Some more LLM-as-judge scores can be build quickly.\n",
    "\n",
    "Guidelines-based judges and scorers use pass/fail natural language criteria to evaluate GenAI outputs. They excel at evaluating:\n",
    "\n",
    "    Compliance: \"Must not include pricing information\"\n",
    "    Style/tone: \"Maintain professional, empathetic tone\"\n",
    "    Requirements: \"Must include specific disclaimers\"\n",
    "    Accuracy: \"Use only facts from provided context\"\n",
    "\n",
    "Benefits\n",
    "\n",
    "    Business-friendly: Domain experts write criteria without coding\n",
    "    Flexible: Update criteria without code changes\n",
    "    Interpretable: Clear pass/fail conditions\n",
    "    Fast iteration: Rapidly test new criteria\n",
    "\n",
    "Another paramater model allows you to choose the model used for the evalaution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bb0bc70-d79e-4ee8-ad41-c0ce7da3f253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Built-in scorers\n",
    "\n",
    "Some scores are already builtin and can be used as is to test the quality of your workflow\n",
    "- RelevanceToQuery : This scorer evaluates if your app's response directly addresses the user's input without deviating into unrelated topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ffee2e-a3d6-4978-b6a9-54ff43a79c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the scorers as a variable so we can re-use them in step 7\n",
    "\n",
    "docs_scorers = [\n",
    "        Guidelines(\n",
    "            name=\"follows_instructions\",\n",
    "            guidelines=\"The generated documents must follow the user_instructions in the request.\",\n",
    "        ),\n",
    "        Guidelines(\n",
    "            name=\"concise_communication\",\n",
    "            guidelines=\"The document MUST be concise and to the point. The document should communicate the key message efficiently without being overly brief or losing important context.\",\n",
    "        ),\n",
    "        Guidelines(\n",
    "            name=\"professional_tone\",\n",
    "            guidelines=\"The document must be in a professional tone.\",\n",
    "        ),\n",
    "        RelevanceToQuery(),  # Checks if email addresses the user's request\n",
    "        reading_level, #Evaluate text complexity\n",
    "        answer_quality,\n",
    "    ]\n",
    "\n",
    "# Run evaluation with predefined scorers\n",
    "eval_results_v1 = mlflow.genai.evaluate(\n",
    "    data=eval_dataset_df,\n",
    "    predict_fn=generate_docs,\n",
    "    scorers=docs_scorers\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dfa50cc-3491-4f68-baf0-609f855e84b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation :\n",
    "The scores are quite good for the limited dataset. \n",
    "The most obvious point seems to keep answer concise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a95487-d7d7-42b8-b119-c77f46d2a2f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2- Exploit traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d52a87-ecc0-458a-82e7-8c71936f80e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2-1 search traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c714fa37-b1da-46a3-99ce-fe941018752d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_traces = mlflow.search_traces(run_id=eval_results_v1.run_id)\n",
    "\n",
    "# eval_traces is a Pandas DataFrame that has the evaluated traces.  The column `assessments` includes each scorer's feedback.\n",
    "eval_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250d0e81-fc84-48f5-8d23-6c69bf9899c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@mlflow.trace\n",
    "def generate_docs_v2(question: str) -> dict:\n",
    "    \"\"\"Generate  the simple_chain model to predict.\"\"\"\n",
    "\n",
    "    question_v2 = f\"\"\"\n",
    "    In the most concise way possible, can you answer the following question? \n",
    "    {question}\n",
    "\n",
    "    \"\"\"    \n",
    "    \n",
    "    response = mlflow_client.predict(\n",
    "        endpoint=\"simple_chain\", # The LLM chain that was created\n",
    "        inputs={\"inputs\": {\"question\": question_v2}} # Some questions you want answers\n",
    "    )\n",
    "\n",
    "    return {\"response\": response[\"predictions\"][0]}\n",
    "\n",
    "\n",
    "# Run evaluation of the new version with the same scorers as before\n",
    "# We use start_run to name the evaluation run in the UI\n",
    "with mlflow.start_run(run_name=\"v2\"):\n",
    "    eval_results_v2 = mlflow.genai.evaluate(\n",
    "        data=eval_dataset_df, # same eval dataset\n",
    "        predict_fn=generate_docs_v2, # new app version\n",
    "        scorers=docs_scorers, # same scorers as step 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9c0ad6d-67b4-4079-b90e-4a23960c4a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we will compare the scores of the v1 and v2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac460c6-8d2d-4128-a690-f38652818fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2-2 Compare v1 et v2 performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "278c80d1-2c42-42c1-84d7-77feda6c0466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch runs separately since mlflow.search_runs doesn't support IN or OR operators\n",
    "run_v1_df = mlflow.search_runs(\n",
    "    filter_string=f\"run_id = '{eval_results_v1.run_id}'\"\n",
    ")\n",
    "run_v2_df = mlflow.search_runs(\n",
    "    filter_string=f\"run_id = '{eval_results_v2.run_id}'\"\n",
    ")\n",
    "\n",
    "# Extract metric columns (they end with /mean, not .aggregate_score)\n",
    "# Skip the agent metrics (latency, token counts) for quality comparison\n",
    "metric_cols = [col for col in run_v1_df.columns\n",
    "               if col.startswith('metrics.') and col.endswith('/mean')\n",
    "               and 'agent/' not in col]\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for metric in metric_cols:\n",
    "    metric_name = metric.replace('metrics.', '').replace('/mean', '')\n",
    "    v1_score = run_v1_df[metric].iloc[0]\n",
    "    v2_score = run_v2_df[metric].iloc[0]\n",
    "    improvement = v2_score - v1_score\n",
    "\n",
    "    comparison_data.append({\n",
    "        'Metric': metric_name,\n",
    "        'V1 Score': f\"{v1_score:.3f}\",\n",
    "        'V2 Score': f\"{v2_score:.3f}\",\n",
    "        'Improvement': f\"{improvement:+.3f}\",\n",
    "        'Improved': '✓' if improvement >= 0 else '✗'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "avg_v1 = run_v1_df[metric_cols].mean(axis=1).iloc[0]\n",
    "avg_v2 = run_v2_df[metric_cols].mean(axis=1).iloc[0]\n",
    "display(f\"Overall average improvement: {(avg_v2 - avg_v1):+.3f} ({((avg_v2/avg_v1 - 1) * 100):+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33bd1aa-5aa5-4e01-b5c1-9ffcfc2cb87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation : \n",
    "While comparing our score, the model improved concisity as we asked but at the expense of the clarity of the text"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Eval_chain_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
