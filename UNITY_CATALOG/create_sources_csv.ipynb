{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc719430-4406-4e7d-86d5-7beb4dbd21ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the Databricks Feature Engineering library\n",
    "# This library provides tools for creating and managing feature tables in Unity Catalog\n",
    "%pip install databricks-feature-engineering\n",
    "\n",
    "# Restart Python kernel to ensure all dependencies are properly initialized\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3072e2f5-c8a8-4677-9b4d-5f05a759cc83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Tables Creation\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will create feature tables from CSV files hosted on GitHub.\n",
    "\n",
    "## Dataset Description\n",
    "The data represents an e-commerce business with the following entities:\n",
    "- **customers**: Customer information and profiles\n",
    "- **orders**: Order transactions and details\n",
    "- **order_items**: Individual items within each order\n",
    "- **products**: Product catalog and specifications\n",
    "- **shipping_zones**: Geographic shipping information\n",
    "\n",
    "## Purpose\n",
    "These feature tables will be used for:\n",
    "- Machine Learning model training\n",
    "- Real-time feature serving\n",
    "- Analytics and reporting\n",
    "- Data lineage tracking through Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7460d3af-d79f-4ace-b7b7-a7859c42c310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../_config/config_unity_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9430fb0b-0ad5-4b96-98e1-399b627cc6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# STEP 1: LOAD CSV FILES FROM GITHUB TO UNITY CATALOG VOLUME\n",
    "# ====================================================================\n",
    "\n",
    "# Define the source location (GitHub raw content URL)\n",
    "prefix = \"https://github.com/O-Faraday/databricks_genai_demo/raw/main/\"\n",
    "\n",
    "# This is the staging area for raw data before creating feature tables\n",
    "path_volume = f\"/Volumes/{catalog}/{schema}/raw_data/\"\n",
    "\n",
    "# List of CSV file names to download and process\n",
    "l_csv_names = [\n",
    "    \"customers\",        # Customer master data (demographics, contact info)\n",
    "    \"orders\",           # Order transactions (order_id, date, status, total)\n",
    "    \"order_items\",      # Line items for each order (product_id, quantity, price)\n",
    "    \"products\",         # Product catalog (name, category, price, inventory)\n",
    "    \"shipping_zones\"    # Shipping zones and rates by geography\n",
    "]\n",
    "\n",
    "# Download each CSV file from GitHub to the Unity Catalog volume\n",
    "for csv_name in l_csv_names:\n",
    "    # Construct source path\n",
    "    source_path = f\"{prefix}data/csv/{csv_name}.csv\"\n",
    "    \n",
    "    # Construct destination path\n",
    "    destination_path = f\"{path_volume}/csv/{csv_name}.csv\"\n",
    "    \n",
    "    # Copy the file using Databricks File System utilities\n",
    "    dbutils.fs.cp(\n",
    "        source_path,\n",
    "        destination_path\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62b9cba-a0ce-4024-b616-9817d3b6d909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load CSV Data into DataFrames\n",
    "\n",
    "### Purpose\n",
    "In this step, we load CSV files from the Unity Catalog volume into Spark DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3cfb13-3c06-4c88-a0ee-4b752f87cf3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# UTILITY FUNCTIONS FOR DATA TRANSFORMATION\n",
    "# ====================================================================\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def addIdColumn(dataframe, id_column_name):\n",
    "    \"\"\"\n",
    "    Add a unique ID column to a DataFrame as the first column.\n",
    "    \n",
    "    This function is essential for creating feature tables because:\n",
    "    - Feature tables MUST have a primary key\n",
    "    - The ID column ensures each row is uniquely identifiable\n",
    "    - monotonically_increasing_id() generates distributed unique IDs\n",
    "    \n",
    "    Args:\n",
    "        dataframe: Input Spark DataFrame\n",
    "        id_column_name: Name for the new ID column (e.g., 'customer_feature_id')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ID column as the first column\n",
    "    \n",
    "    Note:\n",
    "        - IDs are guaranteed unique within a Spark session\n",
    "        - IDs are monotonically increasing but may have gaps\n",
    "        - The ID column is positioned first for better readability\n",
    "    \"\"\"\n",
    "    # Store original column names\n",
    "    columns = dataframe.columns\n",
    "    \n",
    "    # Add ID column \n",
    "    new_df = dataframe.withColumn(id_column_name, monotonically_increasing_id())\n",
    "\n",
    "    # This improves readability and follows best practices for feature tables\n",
    "    return new_df[[id_column_name] + columns]\n",
    "\n",
    "\n",
    "def renameColumns(df):\n",
    "    \"\"\"\n",
    "    Rename columns to be compatible with Unity Catalog Feature Engineering.\n",
    "    \n",
    "    Unity Catalog has naming restrictions:\n",
    "    - Column names cannot contain spaces\n",
    "    - Spaces must be replaced with underscores\n",
    "    \n",
    "    Args:\n",
    "        df: Input Spark DataFrame with potentially non-compliant column names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sanitized column names (spaces replaced by underscores)\n",
    "    \n",
    "    Example:\n",
    "        Input columns: ['Customer Name', 'Order Date', 'Total Amount']\n",
    "        Output columns: ['Customer_Name', 'Order_Date', 'Total_Amount']\n",
    "    \"\"\"\n",
    "    renamed_df = df\n",
    "    \n",
    "    # Iterate through each column and replace spaces with underscores\n",
    "    for column in df.columns:\n",
    "        # withColumnRenamed() creates a new DataFrame with the renamed column\n",
    "        renamed_df = renamed_df.withColumnRenamed(column, column.replace(' ', '_'))\n",
    "    \n",
    "    return renamed_df\n",
    "\n",
    "\n",
    "def compute_customer_features(path_volume, table_name):\n",
    "    \"\"\"\n",
    "    Load and transform CSV data into a feature-ready DataFrame.\n",
    "    \n",
    "    This function represents a feature engineering pipeline:\n",
    "    1. Load raw CSV data from Unity Catalog volume\n",
    "    2. Infer schema automatically from CSV structure\n",
    "    3. Clean column names (remove spaces)\n",
    "    4. Add primary key column for feature table\n",
    "    \n",
    "    Args:\n",
    "        path_volume: path to the csv volume \n",
    "        table_name: Name of the CSV file \n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame with:\n",
    "        - Primary key column: {table_name}_feature_id\n",
    "        - All original columns with sanitized names\n",
    "        - Proper data types inferred from CSV\n",
    "    \n",
    "    Note:\n",
    "        - inferSchema=True: Automatically detects column data types\n",
    "        - header=True: First row contains column names\n",
    "        - sep=\",\": Comma-separated values\n",
    "    \"\"\"\n",
    "    # Load CSV file from Unity Catalog volume\n",
    "    # spark.read.csv() creates a DataFrame from CSV data\n",
    "    df = spark.read.csv(\n",
    "        f\"{path_volume}/csv/{table_name}.csv\",\n",
    "        header=True,        # First row contains column names\n",
    "        inferSchema=True,   # Automatically detect data types (int, string, date, etc.)\n",
    "        sep=\",\"             # Comma delimiter (standard CSV)\n",
    "    )\n",
    "    \n",
    "    # Transform the DataFrame for feature table compatibility\n",
    "    # Step 1: Rename columns to replace spaces with underscores\n",
    "    renamed_df = renameColumns(df)\n",
    "    \n",
    "    # Step 2: Add primary key column as the first column\n",
    "    # Primary key format: {table_name}_feature_id\n",
    "    # Example: 'customers_feature_id', 'orders_feature_id'\n",
    "    return addIdColumn(renamed_df, f'{table_name}_feature_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8041041-9abf-4e20-baa7-6d05833d49d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Feature Tables in Unity Catalog\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502ab59f-2dad-49e5-ba66-57c79728745b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "# Initialize the Feature Engineering Client\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# MAIN LOOP: CREATE FEATURE TABLE FOR EACH CSV FILE\n",
    "# ====================================================================\n",
    "\n",
    "# Process each CSV file and create corresponding feature table\n",
    "for csv_name in l_csv_names:\n",
    "    print(f\"\\n{'='*10}\")\n",
    "    print(f\"Processing table: {csv_name}\")\n",
    "\n",
    "    # Step 1: Load and transform CSV data into feature DataFrame\n",
    "    features_df = compute_customer_features(path_volume=path_volume, table_name=csv_name)\n",
    "    \n",
    "    # Step 2: Create feature table in Unity Catalog\n",
    "    feature_table = fe.create_table(\n",
    "        name=f'{catalog}.{schema}.{csv_name}_feature',     # Full table name: catalog.schema.table_name_feature\n",
    "        primary_keys=f'{csv_name}_feature_id',       # Primary key column (must be unique)\n",
    "        df=features_df,                               # DataFrame containing the feature data\n",
    "        schema=features_df.schema,                    # Schema inferred from DataFrame\n",
    "        description=f'{csv_name} features'            # Human-readable description for documentation\n",
    "    )\n",
    "    \n",
    "    # Step 3: Log the schema for verification and documentation\n",
    "    print(f\"\\nâœ“ Created feature table: {catalog}.{schema}.{csv_name}_feature\")\n",
    "    print(f\"\\nTable schema:\")\n",
    "    print(f\"{features_df.schema}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# RESULT: Feature tables are now available at:\n",
    "# - {catalog}.{schema}.customers_feature\n",
    "# - {catalog}.{schema}.orders_feature\n",
    "# - {catalog}.{schema}.order_items_feature\n",
    "# - {catalog}.{schema}.products_feature\n",
    "# - {catalog}.{schema}.shipping_zones_feature\n",
    "# ===================================================================="
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7286678620309484,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "create_sources_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
