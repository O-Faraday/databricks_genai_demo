# RAG Evaluation Dataset - Deliverables

## What You've Received

This package contains a comprehensive evaluation dataset with **15 queries** specifically designed to test your RAG workflow with the 4 PDF documents from your E-Commerce Electronics Store demo.

## Files Included

### 1. `rag_evaluation_dataset.json`
**Format:** JSON  
**Purpose:** Machine-readable evaluation dataset

Complete structured dataset with:
- Query text
- Query ID and type
- Difficulty level
- Target PDF(s)
- Expected information
- Evaluation criteria

**Use this for:** Automated evaluation scripts and programmatic testing

---

### 2. `rag_evaluation_dataset.csv`
**Format:** CSV  
**Purpose:** Spreadsheet-friendly format

Simplified version for easy viewing and tracking in Excel, Google Sheets, or similar tools.

Columns:
- query_id
- query
- query_type
- difficulty
- target_pdf
- expected_key_points
- notes

**Use this for:** Manual evaluation tracking, team reviews, or importing into evaluation dashboards

---

### 3. `rag_evaluation_documentation.md`
**Format:** Markdown  
**Purpose:** Complete documentation and usage guide

Comprehensive documentation including:
- Query type breakdown
- Difficulty distribution
- Document coverage analysis
- Detailed query explanations
- Evaluation metrics guide
- Usage instructions with code examples
- Testing strategy recommendations
- Common failure patterns

**Use this for:** Understanding the dataset, evaluation planning, and team reference

---

## Quick Start

### For Developers
```python
import json

# Load JSON dataset
with open('rag_evaluation_dataset.json', 'r') as f:
    queries = json.load(f)

# Run evaluation
for item in queries:
    result = your_rag_system.query(item['query'])
    score = evaluate(result, item['expected_info'])
    print(f"Query {item['query_id']}: Score {score}/5")
```

### For Manual Testing
1. Open `rag_evaluation_dataset.csv` in your spreadsheet tool
2. Add columns for: actual_answer, score, notes
3. Run each query through your RAG system
4. Record results and scores
5. Analyze performance by query_type and difficulty

## Dataset Overview

| Metric | Value |
|--------|-------|
| Total Queries | 15 |
| PDFs Covered | 4 (all) |
| Easy Queries | 3 |
| Medium Queries | 9 |
| Hard Queries | 3 |
| Cross-document Queries | 3 |

## Query Coverage by PDF

- **product_catalog.pdf:** 5 queries (product specs, features, compatibility)
- **return_policy.pdf:** 4 queries (returns, refunds, warranties)
- **shipping_guide.pdf:** 5 queries (shipping options, tracking, issues)
- **technical_faq.pdf:** 4 queries (troubleshooting, installation, support)

## What Makes This Dataset Comprehensive

âœ… **Multiple difficulty levels** - From simple facts to complex multi-document reasoning  
âœ… **Diverse query types** - Factual, procedural, troubleshooting, policy questions  
âœ… **Realistic scenarios** - Based on actual e-commerce customer support interactions  
âœ… **Complete coverage** - All 4 PDFs tested with relevant questions  
âœ… **Edge cases included** - Policy exceptions, multi-part questions, cross-document queries  
âœ… **Evaluation criteria** - Clear expectations for what constitutes a good answer  

## Recommended Testing Sequence

1. **Start with Easy (Queries 1, 4, 8)** - Validate basic retrieval
2. **Move to Medium Single-doc (Queries 2, 3, 5, 6, 7, 9, 11, 12, 13)** - Test complexity handling
3. **Finish with Hard Cross-doc (Queries 10, 14, 15)** - Validate advanced capabilities

## Evaluation Scoring Guide

For each query, rate on a **0-5 scale**:

- **5 = Excellent:** Accurate, complete, well-formatted, properly sourced
- **4 = Good:** Correct with minor issues or missing minor details
- **3 = Adequate:** Mostly correct but incomplete or unclear
- **2 = Poor:** Partially correct, missing key information
- **1 = Very Poor:** Mostly incorrect or severely incomplete
- **0 = Failed:** Completely wrong or no answer

**Target Average Score:** 4.0+ indicates a well-performing RAG system

## Questions or Issues?

- Review the detailed documentation in `rag_evaluation_documentation.md`
- Check query-specific details in the JSON file's `evaluation_criteria` field
- Modify queries as needed for your specific use case

## Next Steps

1. âœ… Review all 15 queries
2. âœ… Set up your evaluation environment
3. âœ… Run queries through your RAG system
4. âœ… Track results in the CSV
5. âœ… Analyze performance patterns
6. âœ… Iterate and improve your RAG workflow

---

**Happy Testing! ðŸš€**

*This evaluation dataset is designed specifically for your Databricks GenAI E-Commerce Electronics Store demo and focuses exclusively on the 4 PDF documents.*
