{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f8daaf9-09e0-4383-bd19-435a09545f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create foundation model serving endpoints\n",
    "\n",
    "In this notebook, you will learn how to create model serving endpoints that deploy and serve foundation models.\n",
    "\n",
    "Mosaic AI Model Serving supports the following models:\n",
    "\n",
    "    External models. These are foundation models that are hosted outside of Databricks. Endpoints that serve external models can be centrally governed and customers can establish rate limits and access control for them. Examples include foundation models like OpenAI's GPT-4 and Anthropic's Claude.\n",
    "\n",
    "    State-of-the-art open foundation models made available by Foundation Model APIs. These models are curated foundation model architectures that support optimized inference. Base models, like Meta-Llama-3.1-70B-Instruct, GTE-Large, and Mistral-7B are available for immediate use with pay-per-token pricing. Production workloads, using base or fine-tuned models, can be deployed with performance guarantees using provisioned throughput.\n",
    "\n",
    "Model Serving provides the following options for model serving endpoint creation:\n",
    "\n",
    "    The Serving UI\n",
    "    REST API\n",
    "    MLflow Deployments SDK\n",
    "\n",
    "For creating endpoints that serve traditional ML or Python models, see Create custom model serving endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5421eac9-c31a-4ef8-a00a-4e94e2304199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a foundation model serving endpoint\n",
    "\n",
    "You can create an endpoint that serves fine-tuned variants of foundation models made available using Foundation Model APIs provisioned throughput. See Create your provisioned throughput endpoint using the REST API.\n",
    "\n",
    "For foundation models that are made available using Foundation Model APIs pay-per-token, Databricks automatically provides specific endpoints to access the supported models in your Databricks workspace. To access them, select the Serving tab in the left sidebar of the workspace. The Foundation Model APIs are located at the top of the Endpoints list view.\n",
    "\n",
    "For querying these endpoints, see Use foundation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5e66168-f5fc-4537-b7ad-3cfe4fd698c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create an external model serving endpoint\n",
    "The following describes how to create an endpoint that queries a foundation model made available using Databricks external models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8bfe665-1405-4c84-9f4e-f8824d4f5a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "try:\n",
    "    \n",
    "    endpoint = client.create_endpoint(\n",
    "        name=\"chat\",\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"completions\",\n",
    "                    \"external_model\": {\n",
    "                        \"name\": \"gpt-3.5-turbo\",\n",
    "                        \"provider\": \"openai\",\n",
    "                        \"task\": \"llm/v1/chat\",\n",
    "                        \"openai_config\": {\n",
    "                            \"openai_api_key\": \"{{secrets/llm_secrets/openai_api_key}}\",\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    print(\"Endpoint created !\")\n",
    "    print(f\"Name: {endpoint.get('name')}\")\n",
    "    print(f\"ID: {endpoint.get('id')}\")\n",
    "    print(f\"State: {endpoint.get('state')}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while creating the endpoint: {str(e)}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46190dc-e44d-4285-8cd6-47e875f6caa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to stop this endpoint and use the model created by databricks DBRX-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a724a4e-4ba5-4005-b427-d9ffd8497e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "client = get_deploy_client(\"databricks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040fbc78-38ce-422f-92d9-5ab8cf184793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.delete_endpoint(\"chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c78d213-2811-4683-91c3-d32e51cecf9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    # Create DBRX endpoint DBRX\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=\"chat-dbrx\",\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"dbrx-instruct\",\n",
    "                    \"external_model\": {\n",
    "                        \"name\": \"dbrx-instruct\",\n",
    "                        \"provider\": \"databricks\",\n",
    "                        \"task\": \"llm/v1/chat\",\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Endpoint DBRX created !\")\n",
    "    print(f\"Name: {endpoint.get('name')}\")\n",
    "    print(f\"ID: {endpoint.get('id')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erroorr while creating the endpoint DBRX: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Create foundation model serving endpoints",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
